I'm working on a GenAI Validation platform using Streamlit. The current implementation is functional but fails to provide meaningful, user-understandable results. I need you to **refactor and update the current code** to fix the following problems and implement the required structure:

---

🔁 ISSUE 1: Wrong Metrics Are Being Evaluated
- Even when only a few metrics (e.g., Functional Test) are selected, other metrics (BLEU, G-Eval, etc.) are being run.
- Fix this by **strictly filtering execution only to selected metrics**.
- Metrics must be **mapped per Use Case and Evaluation Focus**, and only the **chosen ones** should run and appear in results.

---

📊 ISSUE 2: Redesign the Evaluation Results Section
The current result is unclear. Instead:
- Show results **per sample** with these columns:
  - `Input`
  - `Expected Output` (Ground Truth)
  - `Generated Output`
  - For each selected metric One column per **selected metric only**, showing:
    - Metric name (e.g., BLEU, Detoxify, G-Eval)
    - Score (0–1 or %)
    - Optional classification (e.g., High/Medium/Low)(e.g., High/Medium/Low if applicable)
- Use an interactive table (e.g., `st.dataframe` or `AgGrid`) to allow sorting/filtering.
- Show overall average scores **only below or separately**, not replacing per-input detail.

---

📁 ISSUE 3: Dataset Handling Must Be Use Case Aware

Uploaded dataset must be validated **dynamically based on the chosen Use Case and Evaluation Focus**. Implement:

- **Required Column Detection**:
  - If the selected use case is Text Generation:
    - Required: `input`, `ground_truth` or `actual`
  - If RAG:
    - Required: `question`, `context`, `generated_answer`, `ground_truth` or `actual`
  - If Code Generation:
    - Required: `prompt`, `generated_code`, `expected_code` or `ground_truth`
  - If Classification:
    - Required: `input`, `label`, `predicted_label`
  - If Image Captioning:
    - Required: `image_path`, `caption`, `ground_truth_caption`

- **Flexible Column Name Matching**:
  - Allow alternate naming (`ground_truth` == `actual`, `expected_output` == `ground_truth`, etc.)
  - If a required column is missing, show a warning:  
    **"Column `ground_truth` (also accepted: `actual`) is missing for selected Use Case."**
  - Highlight what’s missing and prevent evaluation until corrected.

---

🎯 ISSUE 4: Only Show Selected Focus Areas & Metrics

In results, **do not show all evaluation categories (accuracy, toxicity, bias)** by default. Show **only what is selected by the user**:

- If Use Case = RAG and Focus = Toxicity → show only selected metrics under Toxicity (e.g., Detoxify, OpenAI Moderation).
- If Use Case = Code Generation and Focus = Security → show only metrics like Semgrep or Bandit.
- Avoid unnecessary metrics or sections.

---

🎨 ISSUE 5: Remove or Replace Basic Icons
- Icons like the trophy, performance chart, and outdated visuals should be **removed or replaced** with clean typography or modern icons (Lucide/Feather).
- Use minimalistic color-coded text indicators like:
  - ✅ Pass / ❌ Fail
  - 🟢 High Accuracy / 🟡 Medium / 🔴 Low
  - Use color badges instead of emojis if possible.

---

📈 ISSUE 6: Graph Redesign
- Bar and radar charts must still be available, but **as secondary views**, not the primary result.
- Move them to a separate "Visualize Metrics" tab.
- Results tab should **first show per-sample detailed scores**, then allow toggling to graphs.
Use Plotly or Altair for clean, modern bar/radar/line graphs only for the **selected metrics**.

---

✅ INSTRUCTION
Update the current Streamlit app design and code logic based on the above issues and requirements. The platform must:
- Evaluate only selected metrics
- Show per-sample results clearly
- Validate uploaded dataset per use case needs
- Accept alternate column names (ground_truth vs actual)
- Show only selected metric categories (not all)
- Use modern, professional UI/UX

**Update the current Streamlit app design and improve the code according to the above requirements.**
